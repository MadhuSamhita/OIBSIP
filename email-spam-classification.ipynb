{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:50:59.379298Z","iopub.execute_input":"2023-08-25T03:50:59.379776Z","iopub.status.idle":"2023-08-25T03:51:00.391042Z","shell.execute_reply.started":"2023-08-25T03:50:59.379722Z","shell.execute_reply":"2023-08-25T03:51:00.389659Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/email-classifier-dataset/Datasets/spam_normal_emails.csv')  ","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:00.393695Z","iopub.execute_input":"2023-08-25T03:51:00.394151Z","iopub.status.idle":"2023-08-25T03:51:00.503156Z","shell.execute_reply.started":"2023-08-25T03:51:00.394111Z","shell.execute_reply":"2023-08-25T03:51:00.501741Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:00.504786Z","iopub.execute_input":"2023-08-25T03:51:00.505239Z","iopub.status.idle":"2023-08-25T03:51:00.521464Z","shell.execute_reply.started":"2023-08-25T03:51:00.505205Z","shell.execute_reply":"2023-08-25T03:51:00.519825Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                text  spam\n0  Subject: naturally irresistible your corporate...     1\n1  Subject: the stock trading gunslinger  fanny i...     1\n2  Subject: unbelievable new homes made easy  im ...     1\n3  Subject: 4 color printing special  request add...     1\n4  Subject: do not have money , get software cds ...     1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>spam</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Subject: naturally irresistible your corporate...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Subject: the stock trading gunslinger  fanny i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Subject: unbelievable new homes made easy  im ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Subject: 4 color printing special  request add...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Subject: do not have money , get software cds ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Splitting the dataset into features (X) and labels (y)\nX = data['text']\ny = data['spam']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:00.524778Z","iopub.execute_input":"2023-08-25T03:51:00.525223Z","iopub.status.idle":"2023-08-25T03:51:00.535999Z","shell.execute_reply.started":"2023-08-25T03:51:00.525188Z","shell.execute_reply":"2023-08-25T03:51:00.534082Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing using NLTK\ndef preprocess_text(text):\n    # Tokenization\n    tokens = word_tokenize(text)\n    \n    # Remove punctuation and convert to lowercase\n    tokens = [word.lower() for word in tokens if word.isalpha()]\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    \n    return ' '.join(tokens)\n\nX_train_preprocessed = X_train.apply(preprocess_text)\nX_test_preprocessed = X_test.apply(preprocess_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:00.537774Z","iopub.execute_input":"2023-08-25T03:51:00.538239Z","iopub.status.idle":"2023-08-25T03:51:29.325968Z","shell.execute_reply.started":"2023-08-25T03:51:00.538200Z","shell.execute_reply":"2023-08-25T03:51:29.324723Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Creating a CountVectorizer to convert text data into numerical features\nvectorizer = CountVectorizer()\nX_train_vectorized = vectorizer.fit_transform(X_train_preprocessed)\nX_test_vectorized = vectorizer.transform(X_test_preprocessed)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:29.327665Z","iopub.execute_input":"2023-08-25T03:51:29.328164Z","iopub.status.idle":"2023-08-25T03:51:30.456404Z","shell.execute_reply.started":"2023-08-25T03:51:29.328118Z","shell.execute_reply":"2023-08-25T03:51:30.455025Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_train_vectorized.shape,X_test_vectorized.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:30.457918Z","iopub.execute_input":"2023-08-25T03:51:30.458282Z","iopub.status.idle":"2023-08-25T03:51:30.466020Z","shell.execute_reply.started":"2023-08-25T03:51:30.458251Z","shell.execute_reply":"2023-08-25T03:51:30.464773Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"((4582, 30326), (1146, 30326))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Example","metadata":{}},{"cell_type":"code","source":"sample_vectorized = X_train_vectorized[0]\nsample_vectorized_array = sample_vectorized.toarray()\nnon_zero_indices = np.nonzero(sample_vectorized_array)\nfeature_names = vectorizer.get_feature_names_out()\n\n# Displaying non-zero indices and corresponding word frequencies\nfor word_index, frequency in zip(non_zero_indices[1], sample_vectorized_array[non_zero_indices]):\n    word = feature_names[word_index]\n    print(f\"Word: {word}, Frequency: {frequency}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:30.467608Z","iopub.execute_input":"2023-08-25T03:51:30.467996Z","iopub.status.idle":"2023-08-25T03:51:30.512318Z","shell.execute_reply.started":"2023-08-25T03:51:30.467962Z","shell.execute_reply":"2023-08-25T03:51:30.511009Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Word: akhave, Frequency: 1\nWord: allen, Frequency: 1\nWord: ann, Frequency: 1\nWord: approved, Frequency: 4\nWord: billie, Frequency: 1\nWord: bradley, Frequency: 1\nWord: carmen, Frequency: 1\nWord: carol, Frequency: 1\nWord: cc, Frequency: 2\nWord: chavira, Frequency: 1\nWord: click, Frequency: 1\nWord: coats, Frequency: 1\nWord: company, Frequency: 1\nWord: corp, Frequency: 1\nWord: document, Frequency: 1\nWord: ect, Frequency: 26\nWord: ely, Frequency: 1\nWord: enron, Frequency: 1\nWord: epsc, Frequency: 3\nWord: following, Frequency: 1\nWord: form, Frequency: 1\nWord: galvan, Frequency: 1\nWord: gary, Frequency: 1\nWord: hargrave, Frequency: 1\nWord: holloway, Frequency: 3\nWord: hou, Frequency: 13\nWord: indicated, Frequency: 1\nWord: information, Frequency: 1\nWord: jeff, Frequency: 1\nWord: jo, Frequency: 1\nWord: joann, Frequency: 3\nWord: kaminski, Frequency: 2\nWord: kinneman, Frequency: 1\nWord: link, Frequency: 1\nWord: louis, Frequency: 1\nWord: mccumber, Frequency: 1\nWord: michael, Frequency: 1\nWord: michelle, Frequency: 1\nWord: number, Frequency: 2\nWord: payroll, Frequency: 4\nWord: pm, Frequency: 2\nWord: property, Frequency: 1\nWord: received, Frequency: 1\nWord: reclass, Frequency: 1\nWord: reclassification, Frequency: 4\nWord: request, Frequency: 4\nWord: services, Frequency: 1\nWord: sorry, Frequency: 1\nWord: stella, Frequency: 1\nWord: stephen, Frequency: 1\nWord: stewart, Frequency: 1\nWord: subject, Frequency: 3\nWord: view, Frequency: 1\nWord: vince, Frequency: 4\nWord: wolfe, Frequency: 1\nWord: yes, Frequency: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initializing and training the Naive Bayes classifier\nclassifier = MultinomialNB()\nclassifier.fit(X_train_vectorized, y_train)\n\n# Making predictions on the test set\ny_pred = classifier.predict(X_test_vectorized)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:30.514131Z","iopub.execute_input":"2023-08-25T03:51:30.514641Z","iopub.status.idle":"2023-08-25T03:51:30.530755Z","shell.execute_reply.started":"2023-08-25T03:51:30.514599Z","shell.execute_reply":"2023-08-25T03:51:30.529496Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Calculating accuracy and generating classification report\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", report)","metadata":{"execution":{"iopub.status.busy":"2023-08-25T03:51:30.533976Z","iopub.execute_input":"2023-08-25T03:51:30.534330Z","iopub.status.idle":"2023-08-25T03:51:30.553718Z","shell.execute_reply.started":"2023-08-25T03:51:30.534301Z","shell.execute_reply":"2023-08-25T03:51:30.552241Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.9895287958115183\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       856\n           1       0.98      0.98      0.98       290\n\n    accuracy                           0.99      1146\n   macro avg       0.99      0.99      0.99      1146\nweighted avg       0.99      0.99      0.99      1146\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Model Summary\n\nthe Naive Bayes classifier performed very well in this scenario, achieving high accuracy and balanced precision and recall for both spam and ham classes. The results suggest that the classifier is effective at distinguishing between spam and non-spam emails.","metadata":{}},{"cell_type":"markdown","source":"**Accuracy:** The classifier is about 98.95% accurate in predicting whether an email is spam or not.\n\n**Precision:** About 98% of the predicted spam emails were actually spam, and about 99% of the predicted non-spam (ham) emails were actually non-spam.\n\n**Recall (Sensitivity):** The classifier identified about 98% of the actual spam emails and about 99% of the actual non-spam emails.\n\n**F1-Score:** The balanced measure of precision and recall is around 0.98 for spam and 0.99 for non-spam emails.\n\n**Support:** The test set included 856 non-spam (ham) emails and 290 spam emails.\n\n**Macro Avg:** The average performance across both classes (spam and ham) is around 0.99.\n\n**Weighted Avg:** Considering class distribution, the weighted average performance is around 0.99.","metadata":{}}]}